# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sYRdDplrwyZzcWyNEbTlQfkCme-iECHH
"""

# main.py â€” FastAPI backend for PitchDNA MVP
import os
os.environ["NUMBA_THREADING_LAYER"] = "workqueue"
import logging
logging.basicConfig(level=logging.INFO)
import traceback
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import NearestNeighbors
from rapidfuzz import process, fuzz
import uvicorn
from fastapi.responses import JSONResponse
import umap
from fastapi.middleware.cors import CORSMiddleware



# ----------------------------
# Step 1: Load and Prepare Data
# ----------------------------
df = pd.read_csv("final_dataset_pitchDNA.csv")

PITCH_TYPE_MAP = {
    "FF": "Four-Seam Fastball",
    "SL": "Slider",
    "CH": "Changeup",
    "CU": "Curveball",
    "FC": "Cutter",
    "SI": "Sinker",
    "FS": "Splitter",
    "KN": "Knuckleball"}


# Define feature columns for similarity
feature_cols = [
    "avg_speed", "avg_spin", "avg_break_x", "avg_break_z",
    "avg_break_z_induced", "avg_break", "range_speed", "usage_pct"
]
df = df.dropna(subset=feature_cols + ["pitch_type", "name"])

# Pre-fit scaler and KNN using all features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df[feature_cols])
knn = NearestNeighbors(n_neighbors=6, metric='euclidean')
knn.fit(X_scaled)

# Pre-fit UMAP reducer on the full dataset
umap_reducer = umap.UMAP(
    n_neighbors=15,
    min_dist=0.1,
    n_components=2,
    metric="euclidean",
    force_approximation_algorithm=True,
    low_memory=True, n_jobs=1
)
umap_reducer.fit(X_scaled)

# Add UMAP embeddings to DataFrame
umap_embedding = umap_reducer.transform(X_scaled)
df["umap_1"] = umap_embedding[:, 0]
df["umap_2"] = umap_embedding[:, 1]

# Pre-fit UMAP reducer on the full dataset
# umap_reducer = umap.UMAP(
   # n_neighbors=15,
    # min_dist=0.1,
   # n_components=2,
   # metric="euclidean",
   # force_approximation_algorithm=True,  # Optional, safer on macOS
    # low_memory=True, n_jobs=1                      # Avoids numba caching errors
#)

# umap_reducer.fit(X_scaled)

# ----------------------------
# Step 2: Define FastAPI App
# ----------------------------
app = FastAPI(title="PitchDNA Backend")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  
    allow_credentials=True,
    allow_methods=["*"],  
    allow_headers=["*"],
)

@app.get("/")
def read_root():
    return {"message": "PitchDNA backend is live and serving ðŸ”¥"}
@app.get("/umap_data")
def get_umap_data():
    try:
        columns_to_include = [
            "name", "pitch_type", "year", "umap_1", "umap_2", "final_working_URL",
            "avg_speed", "avg_spin", "avg_break_x", "avg_break_z",
            "avg_break_z_induced", "avg_break", "range_speed", "usage_pct"
        ]
        columns_to_include = [col for col in columns_to_include if col in df.columns]
        # Filter out rows with any non-finite values in included columns
        finite_mask = df[columns_to_include].applymap(
            lambda x: not (pd.isna(x) or (isinstance(x, float) and (pd.isnull(x) or not pd.api.types.is_number(x) or x != x or x == float("inf") or x == float("-inf"))))
        ).all(axis=1)
        safe_df = df.loc[finite_mask, columns_to_include]
        data = safe_df.to_dict(orient="records")
        return JSONResponse(content=data)
    except Exception as e:
        print("ðŸ”¥ Error in /umap_data:", str(e))
        import traceback; traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

class PitchQuery(BaseModel):
    name: str
    pitch_type: str
    year: Optional[int] = None  # NEW: year is optional

class PitchFeatures(BaseModel):
    avg_speed: Optional[float] = None
    avg_spin: Optional[float] = None
    avg_break_x: Optional[float] = None
    avg_break_z: Optional[float] = None
    avg_break_z_induced: Optional[float] = None
    avg_break: Optional[float] = None
    range_speed: Optional[float] = None
    usage_pct: Optional[float] = None

class SimilarPitch(BaseModel):
    name: str
    pitch_type: str
    year: int
    distance: float
    matched_name: Optional[str] = None
    umap_1: Optional[float] = None
    umap_2: Optional[float] = None
    video_url: Optional[str] = None  # <-- Add this line

@app.post("/compare_by_player", response_model=List[SimilarPitch])
def compare_by_player(query: PitchQuery):
    logging.info(f"Incoming request - name: {query.name}, pitch_type: {query.pitch_type}, year: {query.year}")
    print("Received query:", query)
    
    def normalize_name(name):
        return name.lower().replace(",", "").replace(".", "").strip()

# Normalize both input and dataset names
    name_map = {normalize_name(name): name for name in df["name"].unique()}
    normalized_input = normalize_name(query.name)

    best_match = process.extractOne(
        normalized_input,
        list(name_map.keys()),
        scorer=fuzz.token_sort_ratio
)

    if not best_match:
        raise HTTPException(status_code=404, detail="No match found.")

    matched_normalized, score = best_match[:2]
    matched_name = name_map[matched_normalized]

    print("Matched name:", matched_name, "Score:", score)

    if score < 70:
        raise HTTPException(status_code=404, detail="No sufficiently close match found.")

    matches = df[
        (df["name"] == matched_name) &
        (df["pitch_type"] == query.pitch_type.upper())
    ]
    print("Matches after name and pitch_type filter:", len(matches))
    if query.year is not None:
        matches = matches[matches["year"] == query.year]
        print("Matches after year filter:", len(matches))

    if matches.empty:
    logging.info(f"Matches found: {len(matches)}")
        print("No matches found for:", matched_name, query.pitch_type, query.year)
        print("Available years for this pitcher/type:", df[(df["name"] == matched_name) & (df["pitch_type"] == query.pitch_type)]["year"].unique())
        raise HTTPException(status_code=404, detail=f"Pitch type (and year) not found for matched pitcher: {matched_name}")

    query_vec = scaler.transform(matches[feature_cols])
    distances, indices = knn.kneighbors(query_vec)

    results = []
    for idx, dist in zip(indices[0][1:], distances[0][1:]):
        row = df.iloc[idx]
        video_url = row.get("final_working_URL")
        if pd.isna(video_url):
            video_url = None
        results.append(SimilarPitch(
            name=row["name"],
            pitch_type=PITCH_TYPE_MAP.get(row["pitch_type"], row["pitch_type"]),
            year=row["year"],
            distance=round(dist, 3),
            matched_name=matched_name,
            umap_1=row.get("umap_1"),
            umap_2=row.get("umap_2"),
            video_url=video_url  # <-- Use cleaned value
        ))
    return results

@app.post("/compare_by_features", response_model=List[SimilarPitch])
def compare_by_features(pitch: PitchFeatures):
    input_dict = pitch.dict()
    selected_features = [k for k, v in input_dict.items() if v is not None]

    if len(selected_features) < 2:
        raise HTTPException(status_code=400, detail="Please provide at least two pitch features.")

    df_filtered = df.dropna(subset=selected_features)
    if df_filtered.empty:
        print("No data for selected features:", selected_features)
        raise HTTPException(status_code=400, detail="No data for selected features.")

    query_vec = [[input_dict[feat] for feat in selected_features]]
    scaler_partial = StandardScaler()
    X_partial = scaler_partial.fit_transform(df_filtered[selected_features])
    query_scaled = scaler_partial.transform(query_vec)

    knn_partial = NearestNeighbors(n_neighbors=5, metric='euclidean')
    knn_partial.fit(X_partial)
    distances, indices = knn_partial.kneighbors(query_scaled)

    results = []
    for idx, dist in zip(indices[0], distances[0]):
        row = df_filtered.iloc[idx]
        video_url = row.get("final_working_URL")
        if pd.isna(video_url):
            video_url = None
        results.append(SimilarPitch(
            name=row["name"],
            pitch_type=PITCH_TYPE_MAP.get(row["pitch_type"], row["pitch_type"]),
            year=row["year"],
            distance=round(dist, 3),
            umap_1=row.get("umap_1"),
            umap_2=row.get("umap_2"),
            video_url=video_url  # <-- Use cleaned value
        ))
    print("Results:", results)
    return results

@app.post("/project_umap")
def project_umap(pitch: PitchFeatures):
    try:
        input_dict = pitch.dict()
        selected_features = [k for k, v in input_dict.items() if v is not None]

        if len(selected_features) < 2:
            raise HTTPException(status_code=400, detail="Please provide at least two pitch features.")

        # Use the same scaler as the main dataset, but only for selected features
        # Fill missing features with the mean from the dataset for consistent scaling
        query_vec = []
        for feat in feature_cols:
            if feat in selected_features:
                query_vec.append(input_dict[feat])
            else:
                # Use mean of that feature from the dataset
                query_vec.append(df[feat].mean())
        print("ðŸ“¥ Received input features:", input_dict)
        print("ðŸ“Š Scaled vector (pre-UMAP):", query_vec)

        query_scaled = scaler.transform([query_vec])

        # Use pre-fitted UMAP to transform the query
        umap_projection = umap_reducer.transform(query_scaled)

        print("ðŸŽ¯ UMAP coordinates:", umap_projection[0])

        return {"umap_1": float(umap_projection[0, 0]), "umap_2": float(umap_projection[0, 1])}
    except Exception as e:
        print("ðŸ”¥ Error in project_umap:", str(e))
        import traceback; traceback.print_exc()
        raise HTTPException(status_code=500, detail="Internal error projecting UMAP")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)

